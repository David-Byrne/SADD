# User Feedback
In an attempt to further improve the user experience (UX) of the frontend of the system, a user feedback process was created. I created a feedback form based off the Questionnaire for User Interface Satisfaction (QUIS), which was originally developed by the University of Maryland [1]. I re-worded certain questions to make them more suitable for our application and I removed others which I felt were irrelevant. I tried to keep the questionnaire as minimal as possible to encourage more people to complete it, whilst still having a sufficient range of questions to generate valuable information.

The feedback form began with a link to the web frontend and a disclaimer encouraging people to skip any questions they felt uncomfortable answering. It was then split into a number of sections, each comprising of a number of questions focusing on a certain part of the software.
* Overall reaction to the software:
  * How good was the software? This was marked on a scale of 0-9 with 0 labelled as terrible and 9 labelled as wonderful.
  * How easy to use was the software? This was marked on a scale of 0-9 with 0 labelled as difficult and 9 labelled as easy.
  * How interesting was the software? This was marked on a scale of 0-9 with 0 labelled as dull and 9 labelled as stimulating.
* Screen
  * Reading characters on the screen. This was marked on a scale of 0-9 with 0 labelled as hard and 9 labelled as easy.
  * Organisation of information. This was marked on a scale of 0-9 with 0 labelled as confusing and 9 labelled as clear.
* Terminology and System Information.
  * Use of terms throughout the system. This was marked on a scale of 0-9 with 0 labelled as inconsistent and 9 labelled as consistent.
  * Computer informs about its progress. This was marked on a scale of 0-9 with 0 labelled as never and 9 labelled as always.
* Learning
  * Learning to operate the system. This was marked on a scale of 0-9 with 0 labelled as difficult and 9 labelled as easy.
  * Exploring new features by trial and error. This was marked on a scale of 0-9 with 0 labelled as difficult and 9 labelled as easy.
  * Supplemental reference materials. This was marked on a scale of 0-9 with 0 labelled as confusing and 9 labelled as clear.
* System capabilities
  * System speed. This was marked on a scale of 0-9 with 0 labelled as too slow and 9 labelled as fast enough.
  * System reliability. This was marked on a scale of 0-9 with 0 labelled as unreliable and 9 labelled as reliable.
  * Designed for all levels of users. This was marked on a scale of 0-9 with 0 labelled as never and 9 labelled as always.
* General feedback
  * List the most negative aspects of the system. This was entered as free form text.
  * List the most positive aspects of the system. This was entered as free form text.

This was created using Google Forms and the circulated using a shareable link. I tried to get a wide cross section of potential users, aiming for diversity in age, gender and computer literacy. The feedback form is completely anonymous however so there was no way for me to link answers to demographics. I felt this was the fairest approach as the small sample size may lead to the demographic information being potentially personally identifiable.


[1] - [Development of an instrument measuring user satisfaction of the human-computer interface](http://delivery.acm.org/10.1145/60000/57203/p213-chin.pdf) by John P. Chin, Virginia A. Diehl and Kent L. Norman (1988)
