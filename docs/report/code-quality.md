# Code Quality

## Testing
Ensuring code functions as intended is a highly important part of the development process. There are 4 main levels of software testing:
* **Unit testing**: This involves testing the functionality of the smallest units of code that are reasonable to test. This is usually done at the function level, or can be done at the class level in an object orientated programming (OOP) environment. Each unit of code may be covered by multiple tests, depending on the cyclomatic complexity of the code. This ensures code with multiple branches or paths is fully tested. Often, these tests are written at the same time as the units of code they are testing. This allows the developer to verify the code works as intended when it was written, reducing the chances of bugs being added to the project. These tests can also be used to check for regressions when new code is being added or existing code is being refactored. Since all tests should be passing when new code is added, any failing tests are flags that previously working code has been broken. An example of a unit test would be checking the "add_to_cart" function of an online shop adds the correct item to the cart. There may need to be multiple variations of this test for different situations, including the cart being previously empty, the cart containing other items, the cart already containing that item etc.
* **Integration testing**: This involves checking the inter-operation of multiple subsystems. We have already verified each individual section works as intended with unit tests. This tries to verify if the components work together. An example of unit tests passing but integration tests failing would be if two subsystems expected different data formats for their communications. A similar issue to this caused the failure of the Mars Climate Orbiter space probe in 1999, when one system calculated a value in United States customary units (imperial) and the second system expected the value it received to be in SI units (metric). There is a wide spectrum of integration testing, from checking the interaction between two classes, to verifying the compatibility of two services in a service orientated architecture based system.
* **System tesing**: Also known as end-to-end testing, this involves testing the entire flow of a use case through the system. It is similar to an integration test in that it tests the interaction between subsystems, however it tests every part needed to fulfil the task rather than just a specific interaction. For an e-commerce website, this could be done by getting a bot to load the web page, sign in, search for an item, add it to the cart and then check out. They should aim to cover the main sequence of steps users are most likely to take.
* **Acceptance testing**: This is the only level of testing that can't entirely be automated, as it requires real users to interact with the system. It is the final round of checks that seek to determine whether the system is fit for purpose. Back when the waterfall software development model was common, this stage would likely involve giving the software to a team of quality assurance engineers (QA) for a period of time to try discover bugs. Once they gave it the all-clear, it could be submitted to the product owner to get them to sign off on the system. Nowadays, with Agile methodologies being much more common and a much shorter average development lifecycle, it often involves deploying changes to a small group of users and relying on analytics and user feedback for results. For example, an online communication tool may rollout changes initially to internal employees to test out (known as "dogfooding"). Assuming the metrics indicate everything is ok, and no major bugs have been reported, they might proceed with a phased rollout. This means the updates are pushed to a small percentage of users first, and then incrementally released to all users over time. If any major issues are discovered, they are usually caught before they impact too many users and the faulty updates can be reverted.